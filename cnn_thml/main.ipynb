{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "from PIL import Image\n",
    "import base64\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "def changeImage(dataUrl):\n",
    "    dataUrl = re.sub('^data:image/.+;base64,','',dataUrl)\n",
    "    image_s = base64.b64decode(dataUrl)\n",
    "    fileName ='test.jpg'\n",
    "    file= open(fileName,'wb')  \n",
    "    file.write(image_s)  \n",
    "    file.close()\n",
    "    img = Image.open('./test.jpg').convert('L')\n",
    "    if img.size[0] != 28 or img.size[1] != 28:\n",
    "        img = img.resize((28, 28))\n",
    "    arr = []\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            pixel = 1.0 - float(img.getpixel((j, i)))/255.0\n",
    "            arr.append(pixel)\n",
    "    arr1 = np.array(arr).reshape((1,784))\n",
    "    print(arr1)\n",
    "    return arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001 #学习率 \n",
    "batch_size = 128 #批尺\n",
    "num_steps = 100 #使用的样本数量\n",
    "display_step = 10 #显示间隔\n",
    "\n",
    "num_input = 784 # MNIST data input (image shape:28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 #随机丢弃一些数据 防止或者减轻拟合\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "    # Reshape to match picture format [Height x Width x Channel]\n",
    "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "pred = tf.argmax(prediction, 1)\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(tf.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    sess.run(init)\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        \n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y,\n",
    "                                                                 keep_prob: 1.0})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss={:.4f}\".format(loss) + \", Training Accuracy={:.3f}\".format(acc))\n",
    "    # Calculate accuracy for 256 MNIST test images\n",
    "    print(sess.run(accuracy, feed_dict={X: mnist.test.images[:500],\n",
    "                                      Y: mnist.test.labels[:500],\n",
    "                                      keep_prob: 1.0}))\n",
    "    save_path = saver.save(sess,\"../tmp/model.ckpt\")\n",
    "    print(\"save success in:\"+save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../tmp/model.ckpt\n",
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.1254902   0.50196078\n",
      "   0.43921569  0.50196078  0.50196078  0.47843137  0.43921569  0.35686275\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.50196078\n",
      "   0.8         1.          0.75294118  1.          1.          0.90980392\n",
      "   0.76862745  0.77647059  0.37254902  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.34509804  0.8         0.82745098  0.53333333  0.43921569  0.50196078\n",
      "   0.50196078  0.47843137  0.49803922  0.89019608  0.74901961  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.4745098   0.58431373  0.29803922  0.          0.          0.\n",
      "   0.          0.          0.0745098   0.81176471  0.75686275  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.18823529  0.74509804  0.72156863  0.39215686  0.          0.          0.\n",
      "   0.          0.          0.29803922  1.          0.81568627  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.0627451   0.67058824  0.85098039  0.88627451  0.70588235  0.0627451\n",
      "   0.          0.          0.          0.37647059  1.          0.56470588\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.29803922  0.83137255\n",
      "   0.85098039  0.76470588  0.0627451   0.          0.          0.88627451\n",
      "   0.74117647  0.29803922  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.0627451   0.78431373  0.8627451   0.74901961  0.12156863  0.57254902\n",
      "   0.85882353  0.65098039  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.0627451   0.82745098  1.          0.95294118\n",
      "   1.          0.70196078  0.18823529  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.18823529  0.67058824  0.98823529\n",
      "   0.98823529  0.89803922  0.18823529  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.0627451   0.56078431  0.81568627  0.85882353\n",
      "   0.96078431  0.83921569  0.69803922  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.35686275  0.9372549   0.85098039  0.81176471\n",
      "   0.56470588  0.12156863  0.74901961  0.81568627  0.23529412  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.58039216  0.83529412  1.          0.64705882\n",
      "   0.1254902   0.          0.          0.56470588  0.76862745  0.34509804\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.49411765  0.94509804  0.8         0.4\n",
      "   0.          0.          0.          0.          0.49019608  0.81176471\n",
      "   0.34509804  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.1254902   0.68627451  0.84705882\n",
      "   0.18823529  0.          0.          0.          0.          0.\n",
      "   0.72156863  0.71764706  0.23529412  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.45490196\n",
      "   0.81176471  0.51764706  0.          0.          0.          0.          0.\n",
      "   0.1254902   0.89019608  0.72156863  0.0627451   0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.37647059  0.90196078  0.69411765  0.          0.          0.          0.\n",
      "   0.          0.74901961  0.74901961  0.53333333  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.90196078  0.79607843  0.34509804  0.          0.          0.\n",
      "   0.62745098  0.85882353  0.82745098  0.1254902   0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.43921569  0.81568627  0.88627451  0.18823529  0.25098039\n",
      "   0.88627451  0.8627451   0.94117647  0.31372549  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.59215686  0.84705882  0.89019608\n",
      "   0.8627451   0.79215686  0.65098039  0.18823529  0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.43921569\n",
      "   0.52941176  0.64705882  0.35686275  0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]]\n",
      "[8]\n"
     ]
    }
   ],
   "source": [
    "#load model and prediction\n",
    "with tf.Session() as sess:\n",
    "#     train()\n",
    "    saver.restore(sess, \"../tmp/model.ckpt\")\n",
    "    arr1 = changeImage(\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAYAAAByDd+UAAACvUlEQVRIS+1WPUiyURg9LhYhEUImStqgU2Au4WJE0GCp4NAQBZqQhSCUCSq52B85GDg4OjSUNCX2Q6M4uClESK4SIQT9OBQRQX48FxTD6r19RnzDd8FF7/uce84957yKarVaDb+4RP8Bf1rtf1vScrmM3d1d9PT0oFqtMvJisRhdXV3o7e3F7OysoCDcDMPhME5OTlAoFDA6OopsNsuG9/X1QaPR4O3tDYuLi3A4HF+CcgEeHBwgGo3i5eUFZrMZcrn8HcPX11ckEgkGnMlk2gfc3t7G9fU1JiYmYLFYWgbe3d1hfHwc3d3dDeafoQoyPD8/x8rKCrurqakpzM/Pt8wiqa1WK4aGhkD7v1qCgGQOl8uFi4sLPD8/Q6fTYW9vjxmnvshITqcTS0tLiMVi7QHWnyYWwWAQdF8ymQzHx8cNUJVKBa1WC6/X+6HkzScQZNi8mdiaTCbc3t6iv7+fRWRnZwdnZ2cYHh5GMpn8uVjUJxHoyMgIisUiJBIJHh8fGauNjQ3o9fqfByRTbG1tIZVKNXJ4enrKBUYPcEtKzNbW1ljOnp6eUKlUoFarWfaOjo4EmdU3cAHu7+/D4/Ews1CNUdOQQebm5vDw8IB4PA6j0cgFKghIQMTq8vISMzMzTM6BgQE2fHl5Gfl8nh1menq6fcD19XXmQrL8wsIC+zQvMsrh4SECgUD7gPRmINlKpRKoSz9yIEVEJBIhFAq1LymVNd0dlfXm5maLXHQgisfg4CA7UHPz/FW1UahXV1dZoNPpdMuMsbEx9p3BYEAkEuG6P8FYKBQKKJVK+Hy+d3dE7szlcujs7GQVVzcRD+qXLqUiJqb39/csa5RFu93O3hxUb/SbzWbjwWnsEYyF2+1uhL2jowNSqZSF3e/3c7fLt8ubmN7c3ODq6gqTk5Nc/10+oy3I8Ft6cWz+dcA/8zN3vADs60gAAAAASUVORK5CYII=\")\n",
    "# #     print(sess.run(accuracy, feed_dict={X: mnist.test.images[:500],\n",
    "# #                                       Y: mnist.test.labels[:500],\n",
    "# #                                       keep_prob: 1.0}))\n",
    "    print(sess.run(pred,feed_dict={X:arr1,keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
